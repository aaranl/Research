{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables\n",
    "\n",
    "\\begin{array}{llll}\n",
    "\n",
    "\\begin{array}{llll}\n",
    "\\text{STM} & \\text{MTM} & \\text{LTM} & \\text{F}_0 \\rightarrow \\text{F}_2 \\text{ signal} & \\text{F}_2 \\rightarrow \\text{F}_1 \\text{ signal} \\\\\n",
    "\\hline\n",
    "I_i - \\text{F}_0 \\text{ (input)} & \\Delta_{ij} - \\text{Phasic} & T_{ij} - \\text{F}_0 \\rightarrow \\text{F}_2 & S_j - \\text{Phasic} & \\sigma_i - \\text{Total} \\\\\n",
    "x_i - \\text{F}_1 \\text{ (matching)} & \\delta_{ij} - \\text{Tonic} & T_{ji} - \\text{F}_2 \\rightarrow \\text{F}_1 & \\Theta_j - \\text{Tonic} & \\\\\n",
    "y_j - \\text{F}_2 \\text{ (coding)} & & & T_j - \\text{Total} & \\\\\n",
    "\\end{array}\n",
    "\\end{array}\n",
    "\n",
    "* Phasic regards to rapid, temporary responses that have a quick onset and quick decay, they are related to immediate changes, or short-term activity\n",
    "* Tonic regards to sustained, longer-lasting responses that have a slower onset and slower decay, they are related to maintained activity or baseline states.\n",
    "* These terms come from neuroscience where phasic activity refers to brief burst of neural firing, and tonic activity refers to sustained, regular firing patterns. \n",
    "* It's important to note that the layered architecture differs from ART models where inputs must first pass through F₁ to reach F₂. In the dART network, the coding field F₂ receives input directly from F₀, retaining the bottom-up/top-down matching process at F₁ only to determine whether an active code meets the vigilance matching criterion.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "\\begin{align*}\n",
    "&\\text{Number of input components}, \\quad i = 1 \\ldots M \\\\\n",
    "&\\text{Number of coding nodes}, \\quad j = 1 \\ldots N \\\\\n",
    "&\\text{Signal rule}, \\quad \\alpha \\in (0,1) \\text{ (choice-by-difference) or } \\alpha > 0 \\text{ (Weber law)} \\\\\n",
    "&\\text{CAM rule}, \\quad p \\text{ (power law) and } Q \\text{ (Q-max)}, \\text{ with } p \\rightarrow \\infty \\text{ or } Q = 1 \\text{ for choice} \\\\\n",
    "&\\text{Learning rate}, \\quad \\beta \\in [0,1] \\\\\n",
    "&\\text{Vigilance}, \\quad \\rho \\in [0,1] \\\\\n",
    "&\\text{A set of small, positive, random numbers, for initial } T_{ij} \\text{ values}, \\quad \\eta_{ij} = 0^+\\\\\n",
    "\\end{align*}\n",
    "\n",
    "#### Signal Rule\n",
    "\n",
    "* The total signal $T_J$ from the dART input field $F_0$ to the $j^{th}$ $F_{2}$ node is a function of the form: $T_j = T_j(y_j) = g_j(S_j(y_j), \\Theta_j(y_j))$\n",
    "    - This can either be derived to a choice-by-difference variant: $T_j = \\|[x \\wedge (1\\vec{} - \\tau^{bu}_j) - \\Delta_j]^+\\|_1 + (1 - \\alpha)\\|[\\tau^{bu}_j - \\delta_j]^+\\|_1, \\quad 0 < \\alpha < 1$\n",
    "    - This can also be derived to the Webers law variant $T_j = \\frac{\\|[x \\wedge (1\\vec{} - \\tau^{bu}_j) - \\Delta_j]^+\\|_1}{\\alpha + d - \\|[\\tau^{bu}_j - \\delta_j]^+\\|_1}, \\quad \\alpha > 0$\n",
    "\n",
    "* In terms of showing the rest I will just write it out as  $T_j = g_j(S_j, \\Theta_j)$ and I will show $S_j$ and $\\Theta_j$\n",
    "\n",
    "#### Content-Addressable-Memory (CAM) Rule\n",
    "\n",
    "* Activity $y \\equiv (y_{1}... y_{i}... y_{N})$ at a competitive coding field $F_{2}$ is stored as a content-addressable memory (CAM)\n",
    "    - Content Addressable Memory (CAM) is a special type of computer memory system where data is accessed based on its content rather than its physical location or address\n",
    "\n",
    "$$y^{(F2)}_j = \\begin{cases} \n",
    "\\frac{(T_j)^p}{\\sum_{\\lambda \\in \\Lambda} (T_\\lambda)^p} & \\text{if } j \\in \\Lambda \\\\\n",
    "0 & \\text{otherwise} \n",
    "\\end{cases} \\\\\n",
    "\\text{ such that } \\|y^{(F2)}\\|_1 = 1 \\text{ and } p > 0$$\n",
    "\n",
    "* In the future I will write this as $y_j = f_j(T_1,\\ldots,T_N)$\n",
    "* Λ is the set of F₂ nodes j where $T_j$ exceeds a threshold\n",
    "* If node j is in the set $\\Lambda$ \n",
    "    - Calculate $y_j$ using the normalized power rule\n",
    "    - Content Addressable Memory (CAM) is a special type of computer memory system where data is accessed based on its content rather than its physical location or address\n",
    "\n",
    "\n",
    "#### First Iteration n=1\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{MTM depletion } \\Delta_{ij} = \\delta_{ij} = 0 \\\\\n",
    "&\\text{F}_0 \\rightarrow \\text{F}_2 \\text{ threshold } T_{ij} = \\eta_{ij} \\\\\n",
    "&\\text{F}_2 \\rightarrow \\text{F}_1 \\text{ threshold } T_{ji} = 0 \\\\\n",
    "&\\text{Input } I_i = I_{i}^{(1)}\n",
    "\\end{align*}$$\n",
    "\n",
    "#### Reset: new STM steady-state at $F_{2}$ and $F_{1}$\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\textbf{F}_0 \\rightarrow \\textbf{F}_2 \\text{ signal:} \\\\\n",
    "&\\text{Phasic: } S_j = \\sum_{i=1}^M [I_i \\wedge (1 - T_{ij}) - \\Delta_{ij}]^+ \\\\\n",
    "&\\text{Tonic: } \\Theta_j = \\sum_{i=1}^M [T_{ij} - \\delta_{ij}]^+ \\\\\n",
    "&\\text{Total: } T_j = g_j(S_j, \\Theta_j) \\\\\n",
    "&\\textbf{F}_2 \\text{ activation: } y_j = f_j(T_1,\\ldots,T_N) \\\\\n",
    "&\\textbf{F}_2 \\rightarrow \\textbf{F}_1 \\text{ signal: } \\sigma_i = \\sum_{j=1}^N [y_j - T_{ji}]^+ \\\\\n",
    "&\\textbf{F}_1 \\text{ activation: } x_i = I_i \\wedge \\sigma_i\n",
    "\\end{align*}$$\n",
    "\n",
    "* The y_j and T_j calculation may seem circular but they go into why this is circular but I don't completely understand what it accomplishes.\n",
    "    - Page 1477\n",
    "\n",
    "\n",
    "#### MTM Depletion: $F_2$ sites refractory on the time scale of search\n",
    "$$\\begin{align*}\n",
    "&\\text{Phasic: } \\Delta^{old}_{ij} = \\Delta_{ij} \\\\\n",
    "&\\quad\\quad\\quad\\, \\Delta_{ij} = \\Delta^{old}_{ij} \\vee (I_i \\wedge [y_j - T_{ij}])^+ \\\\\n",
    "&\\text{Tonic: } \\delta^{old}_{ij} = \\delta_{ij} \\\\\n",
    "&\\quad\\quad\\quad\\, \\delta_{ij} = \\delta^{old}_{ij} \\vee (y_j \\wedge T_{ij})\n",
    "\\end{align*}$$\n",
    "\n",
    "* The MTM depletion formulas are defined to help with the following:\n",
    "    - Phasic Component: Tracks mistatches between input and category prediction and will accumlate when there are mismatches.\n",
    "    - Tonic Component: Tracks recent category activations which helps prevent overruse of the same categories and accumlates when there is overuse. \n",
    "    - These accumlate more and more as time goes on and are reset back to 0 once a search is over.\n",
    "\n",
    "#### Reset or Resonance(Learning):  \n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{If } \\sum_{i=1}^M x_i < \\rho \\sum_{i=1}^M I_i, \\text{ go to (6) Reset} \\\\\n",
    "&\\text{If } \\sum_{i=1}^M x_i \\geq \\rho \\sum_{i=1}^M I_i, \\text{ go to (9) Resonance}\n",
    "\\end{align*}$$\n",
    "\n",
    "* This is how the vigilance is definied because it is distributed, it is the same logic as other vigilance checks just written differently because of normalization and input size differences. \n",
    "\n",
    "#### Learning\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{Old values: } T^{old}_{ij} = T_{ij}, \\, T^{old}_{ji} = T_{ji}, \\, \\sigma^{old}_i = \\sigma_i \\\\\n",
    "\\\\\n",
    "&\\text{Increase } F_0 \\rightarrow F_2 \\text{ threshold:} \\\\\n",
    "&T_{ij} = T^{old}_{ij} + \\beta[y_j - T^{old}_{ij} - I_i]^+ \\\\\n",
    "\\\\\n",
    "&\\text{Increase } F_2 \\rightarrow F_1 \\text{ threshold:} \\\\\n",
    "&T_{ji} = T^{old}_{ji} + \\beta[\\sigma^{old}_i - I_i]^+ \\frac{\\sigma^{old}_i}{[y_j - T^{old}_{ji}]^+} \\\\\n",
    "\\\\\n",
    "&\\text{Decrease } F_2 \\rightarrow F_1 \\text{ signal: } \\\\\n",
    "&\\sigma_i = \\sigma^{old}_i - \\beta[\\sigma^{old}_i - I_i]^+ \\\\\n",
    "\\\\\n",
    "&\\text{MTM recovery: } \\Delta_{ij} = \\delta_{ij} = 0\n",
    "\\end{align*}$$\n",
    "\n",
    "* Increasing the $T_{ij}$ Thresehold serves a couple purposes, but it helps prevent nodes to responding to inputs that are not similar, and makes them become more specific to the learned patterns as learning continues.\n",
    "* Increasing $T_{ji}$ helps refine the matching process, and ability to reconstuct learned patterns. \n",
    "\n",
    "#### Next pattern or n += 1\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{New input: } I_i = I^{(n)}_i \\\\\n",
    "&\\text{New F}_1 \\text{ activation: } x_i = I_i \\wedge \\sigma_i\n",
    "\\end{align*}$$\n",
    "\n",
    "#### Overall Takeaways\n",
    "\n",
    "* Architecture\n",
    "    - $F_0$ -> $F_2$ <-> $F_1$\n",
    "    - Direct $F_0$ -> $F_2$ connection differs from traditional ART\n",
    "\n",
    "* Components\n",
    "    - $F_0$ receives input patterns $I_i$ and directly connects to $F_2$\n",
    "    - $F_2$ forms distributed codes $y_j$ and can have multiple nodes be active as a part of the $\\Lambda$ set\n",
    "    - $F_1$ compares input with predictions and computes the match scores\n",
    "    - Uses choice-by-difference activation function for $T_j$\n",
    "\n",
    "* Memory\n",
    "    - STM holds immediate activations in $x_i$ and $y_j$ and the state of the current system\n",
    "    - MTM holds the depletion parameters $\\Delta$ and $\\delta$ which:\n",
    "        - Prevent perseveration\n",
    "        - Create refractory periods\n",
    "        - Encourage exploration of different categories\n",
    "    - LTM holds thresholds $T_{ij}$ and $T_{ji}$ for stable learning\n",
    "\n",
    "* Processing Cycle\n",
    "    - Input presentation\n",
    "    - Category activation\n",
    "    - Match check using vigilance parameter $\\rho$\n",
    "    - Reset or Resonance decision\n",
    "    - Learning if resonance achieved\n",
    "\n",
    "* The learning process is as defined above but is simplified but we can see that the overall steps gone through are exactly the same as a simple Fuzzy ART system..\n",
    "\n",
    "* There is a ton more math that goes into a system of this sort as it has to interact with multiple systems and there needs to be a balance between all these systems with communication.\n",
    "\n",
    "* There are a couple things that I still don't completely comprehend their purpose but it's related to communication between systems and making sure that there are not times in which the wrong information will be sent if two systems are not completely synced up on a time scale. \n",
    "    - Even without completely comprehending them, I can understand that the above process works as intended as long as there are no time scale mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
